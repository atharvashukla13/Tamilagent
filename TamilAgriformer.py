# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jxBN8JIPfRTvilHh9VbkFKkeC8Gezlal
"""

import json
import re
import nltk
from sentence_transformers import SentenceTransformer, InputExample, losses
from torch.utils.data import DataLoader
from google.colab import files
import pickle
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

nltk.download('punkt')

def clean_text_tamil(text):
    # Lowercase not needed, Tamil has no case
    text = re.sub(r'[^அ-ஹஃா-௺\s]', '', text)  # keep only Tamil letters & spaces
    tokens = nltk.word_tokenize(text)
    return " ".join(tokens)

uploaded = files.upload()  # Select your Tamil JSON (AppStructure_tamil.json)

import pandas as pd
with open("TamilKnowledgeBase.json", "r", encoding="utf-8") as f:
    data = json.load(f)

train_examples = []

for feature in data['features']:
    page_name = feature['page_name']
    for keyword in feature['keywords']:
        # cleaned_keyword = clean_text_tamil(keyword) # Removed call to clean_text_tamil
        # Tamil InputExample
        train_examples.append(InputExample(texts=[keyword, page_name], label=1.0)) # Use original keyword
print(f"✅ Total Tamil training examples: {len(train_examples)}")

model = SentenceTransformer("l3cube-pune/indic-sentence-similarity-sbert")

# Example Tamil user query
test_sentence = "என்டிவிஐ மூலம் பயிர்களின் ஆரோக்கியத்தை மதிப்பிடலாம்"

# Prepare candidates from Tamil JSON
candidates = []
candidate_texts = []

for feature in data['features']:
    page_name = feature['page_name']
    for keyword in feature['keywords']:
        candidates.append(page_name)
        candidate_texts.append(keyword)

# Encode candidates and query
candidate_embeddings = model.encode(candidate_texts)
test_embedding = model.encode([test_sentence])

from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

similarities = cosine_similarity(test_embedding, candidate_embeddings)
best_idx = np.argmax(similarities)
predicted_page = candidates[best_idx]

print(f"Predicted page: {predicted_page}")